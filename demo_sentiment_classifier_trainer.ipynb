{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from twitter_nlp_toolkit.twit_module import twit_module\n",
    "from twitter_nlp_toolkit.file_fetcher import file_fetcher\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk = 1 # Fraction of data to train on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we download some training and validation data.\n",
    "\n",
    "The training data is the semi-supervised Sentiment140 dataset, taken form here: https://www.kaggle.com/kazanova/sentiment140\n",
    "\n",
    "The validation data is hand-labeled airline customer feedback taken from https://www.figure-eight.com/data-for-everyone/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloader initialized\n"
     ]
    }
   ],
   "source": [
    "downloader = file_fetcher.downloader(using_notebook=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 99% [81100800 / 81334274] bytes            \r\n"
     ]
    }
   ],
   "source": [
    "# Training data \n",
    "downloader.download_file(\"https://www.dropbox.com/s/5zr4e84x83vevbt/training.1600000.processed.noemoticon.csv.zip?dl=1\",\"1_6_m_tweets.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Validation data\n",
    "downloader.download_file('https://www.dropbox.com/s/440m6x07bjg6c0h/Tweets.zip?dl=1',\"Tweets.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 99% [932806656 / 933152569] bytes            \r\n"
     ]
    }
   ],
   "source": [
    "# Pickled model\n",
    "# Note that this model has only been trained on 1% of the training data\n",
    "# Update when better-trained model is available\n",
    "downloader.download_file(\"https://www.dropbox.com/s/owpldku3kk7aaqj/tweet_classifier_001.zip?dl=1\",\"tweet_classifier_001.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#unzip the pickle file\n",
    "with ZipFile('tweet_classifier_001.zip', 'r') as zipObj:\n",
    "   # Extract all the contents of zip file in current directory\n",
    "   zipObj.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"1_6_m_tweets.zip\", encoding='latin-1',\n",
    "                         names=['Labels', 'Index', 'Time', 'Query', 'Handle', 'Text'])\n",
    "\n",
    "test_data = pd.read_csv('Tweets.zip', header=0, names=['Index', 'Sentiment', 'Sentiment_confidence',\n",
    "                                                                'Negative_reason', 'Negative_reason_confidence',\n",
    "                                                                'Airline', 'Airline_sentiment_gold', 'Handle',\n",
    "                                                                'Negative_reason_gold', 'Retweet_count', 'Text',\n",
    "                                                                'Tweet_coord', 'Time', 'Location', 'Timezone'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['Labels'] = (test_data['Sentiment'] == 'positive') * 2\n",
    "test_data['Labels'] = test_data['Labels'] + (test_data['Sentiment'] == 'neutral') * 1\n",
    "test_data['Labels'] = test_data['Labels'] / 2\n",
    "\n",
    "train_data['Labels'] = np.array(train_data['Labels'])/4\n",
    "\n",
    "\n",
    "# For debugging, it is possible to choose only a fraction of the data for speed\n",
    "if chunk < 1:\n",
    "    train_data = train_data.reindex(np.random.permutation(train_data.index))\n",
    "    train_data = train_data[0:int(chunk * len(train_data))]\n",
    "\n",
    "# Remove unlabeled test data\n",
    "test_data.set_index('Labels')\n",
    "test_data = test_data[test_data.Labels != 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\strix\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Executing this cell takes about 20min on a laptop\n",
    "\n",
    "Classifier = twit_module.SentimentAnalyzer(bow_param={}, lstm_param=None, glove_param=None)\n",
    "Classifier.fit(train_data['Text'], train_data[\"Labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.791\n"
     ]
    }
   ],
   "source": [
    "test_data['Labels'] = (test_data['Sentiment'] == 'positive') * 2\n",
    "\n",
    "print(\"Test Accuracy: %.3f\" % sklearn.metrics.accuracy_score(test_data['Labels']/2, Classifier.predict(test_data['Text']).reshape(-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'twit_module'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-d1c24a7396bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mEnsembledClassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpkl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tweet_classifier_001.pkl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'twit_module'"
     ]
    }
   ],
   "source": [
    "EnsembledClassifier = pkl.load(open('tweet_classifier_001.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test Accuracy: %.3f\" % sklearn.metrics.accuracy_score(test_data['Labels']/2, EnsembledClassifier.predict(test_data['Text']).reshape(-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EnsembledClassifier.predict(['I am happy', 'I am sad', 'I am cheerful', 'I am mad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `os.mk_dir` not found.\n"
     ]
    }
   ],
   "source": [
    "?os.mk_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "nteract": {
   "version": "0.21.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
